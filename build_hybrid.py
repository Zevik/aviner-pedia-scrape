#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
×‘×•× ×” ×“××˜×”-×‘×™×™×¡ ××©×•×œ×‘: ×§×•×¨× ×§×‘×¦×™× ×›××• build.py + ×§×˜×’×•×¨×™×•×ª ×-XML
"""
import os
import re
import sqlite3
from pathlib import Path
from collections import defaultdict
import pandas as pd
from bs4 import BeautifulSoup
import datetime
import logging
import xml.etree.ElementTree as ET

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def extract_categories_from_xml(xml_path):
    """
    ×—×•×œ×¥ ×§×˜×’×•×¨×™×•×ª ×-XML ×œ×›×œ ×¢××•×“
    ××—×–×™×¨ dict: {title: [categories]}
    """
    logging.info("ğŸ“– ×§×•×¨× ×§×˜×’×•×¨×™×•×ª ×-XML...")
    tree = ET.parse(xml_path)
    root = tree.getroot()
    ns = {'mw': 'http://www.mediawiki.org/xml/export-0.11/'}

    categories_map = {}
    pages = root.findall('.//mw:page', ns)

    for page in pages:
        title_elem = page.find('mw:title', ns)
        if title_elem is None:
            continue

        title = title_elem.text

        # ××“×œ×’×™× ×¢×œ ×¢××•×“×™ ××¢×¨×›×ª
        if title.startswith('×§×˜×’×•×¨×™×”:') or title.startswith('Category:') or \
           title.startswith('×ª×‘× ×™×ª:') or title.startswith('Template:'):
            continue

        # ××•×¦××™× ××ª ×”×’×¨×¡×” ×”××—×¨×•× ×”
        revisions = page.findall('mw:revision', ns)
        if not revisions:
            continue

        latest_revision = revisions[-1]
        text_elem = latest_revision.find('.//mw:text', ns)
        if text_elem is None or text_elem.text is None:
            continue

        wikitext = text_elem.text

        # ×—×™×œ×•×¥ ×§×˜×’×•×¨×™×•×ª
        cat_pattern = r'\[\[(?:×§×˜×’×•×¨×™×”|Category):([^\]|]+)(?:\|[^\]]*)?\]\]'
        categories = []
        for match in re.finditer(cat_pattern, wikitext, re.IGNORECASE):
            cat_name = match.group(1).strip()
            categories.append(cat_name)

        if categories:
            categories_map[title] = categories

    logging.info(f"âœ… × ××¦××• ×§×˜×’×•×¨×™×•×ª ×œ-{len(categories_map)} ×¢××•×“×™×")
    return categories_map

def categorize_with_xml(title, xml_categories):
    """
    ×§×‘×™×¢×ª ×§×˜×’×•×¨×™×” ×•×ª×ª-×§×˜×’×•×¨×™×” ×¢×œ ×¤×™ ×§×˜×’×•×¨×™×•×ª ×-XML
    """
    # ×‘×¨×™×¨×ª ××—×“×œ
    category = '×›×œ×œ×™'
    subcategory = '×›×œ×œ×™'

    # ×§×‘×™×¢×ª ×§×˜×’×•×¨×™×” ×¨××©×™×ª ××”×›×•×ª×¨×ª
    if '(×•×™×“××•)' in title or '(×©×™×¢×•×¨)' in title:
        category = '×•×™×“××•'
    elif '(××××¨)' in title:
        category = '××××¨×™×'
    elif '(×©×•"×ª)' in title or '(×©×•_×ª)' in title:
        category = '×©×•"×ª ×”×œ×›×”'
    elif '(×¡×“×¨×”)' in title:
        category = '×¡×“×¨×•×ª'

    # ××™×¤×•×™ ×§×˜×’×•×¨×™×•×ª ×œ×ª×ª×™-×§×˜×’×•×¨×™×•×ª
    priority_map = {
        # ×¡×“×¨×•×ª
        '××•×¨×•×ª ×”×§×•×“×©': '××•×¨×•×ª ×”×§×•×“×©',
        '××•×¨×•×ª ×”×ª×©×•×‘×”': '××•×¨×•×ª ×”×ª×©×•×‘×”',
        '×¡×¤×¨ ××•×¨×•×ª': '×¡×¤×¨ ××•×¨×•×ª',
        '×¢×™×Ÿ ××™×”': '×¢×™×Ÿ ××™×”',
        '×›×•×–×¨×™': '×›×•×–×¨×™',
        '×©××•× ×” ×¤×¨×§×™×': '×©××•× ×” ×¤×¨×§×™× ×œ×¨××‘×',
        '×ª×¤××¨×ª ×™×©×¨××œ': '×ª×¤××¨×ª ×™×©×¨××œ - ××”×¨"×œ',

        # ×©×•"×ª ×¡×¤×¦×™×¤×™
        '×©×•"×ª ×¡××¡': '×©×•"×ª ×¡××¡',
        '×¡××¡': '×©×•"×ª ×¡××¡',
        'SMS': '×©×•"×ª ×¡××¡',

        # × ×•×©××™×
        '××§×˜×•××œ×™×”': '××§×˜×•××œ×™×”',
        '×–×•×’×™×•×ª ×•××©×¤×—×”': '×–×•×’×™×•×ª ×•××©×¤×—×”',
        '×–×•×’×™×•×ª': '×–×•×’×™×•×ª ×•××©×¤×—×”',
        '××©×¤×—×”': '×–×•×’×™×•×ª ×•××©×¤×—×”',
        '× ×™×©×•××™×Ÿ': '×–×•×’×™×•×ª ×•××©×¤×—×”',
        '×—×ª×•× ×”': '×–×•×’×™×•×ª ×•××©×¤×—×”',
        '××“×™× ×ª ×™×©×¨××œ': '××“×™× ×ª ×™×©×¨××œ',
        '××¨×¥ ×™×©×¨××œ': '××“×™× ×ª ×™×©×¨××œ',
        '×¦×”"×œ': '××“×™× ×ª ×™×©×¨××œ',
        '××•×¡×¨ ×•××™×“×•×ª': '××•×¡×¨ ×•××™×“×•×ª',
        '××•×¡×¨': '××•×¡×¨ ×•××™×“×•×ª',
        '××•×¢×“×™×': '××•×¢×“×™×',
        '×—×’×™×': '××•×¢×“×™×',
        '×©×‘×ª': '××•×¢×“×™×',
        '×¤×¡×—': '××•×¢×“×™×',
        '×ª×¤×™×œ×”': '×ª×¤×™×œ×”',
        '×ª×¤×™×œ×•×ª': '×ª×¤×™×œ×”',
        '×‘×¨×›×•×ª': '×ª×¤×™×œ×”',
        '×ª×•×¨×”': '×ª×•×¨×”',
        '×œ×™××•×“ ×ª×•×¨×”': '×ª×•×¨×”',
        '×—×™× ×•×š': '×—×™× ×•×š',
        '×××•× ×”': '×××•× ×”',
        '××™×•×—×“×™×': '××™×•×—×“×™×',
        '×”×œ×›×”': '×”×œ×›×”',
        '××•×¨×•×ª': '××•×¨×•×ª',
    }

    # ×× ××™×Ÿ ×§×˜×’×•×¨×™×” XML, ×¢×‘×•×¨ ×©×•"×ª × ×©×ª××© ×‘×‘×¨×™×¨×ª ××—×“×œ
    if not xml_categories and category == '×©×•"×ª ×”×œ×›×”':
        subcategory = '×©×•"×ª ×œ×¤×™ × ×•×©×'
        # ×–×™×”×•×™ SMS
        if '×¡××¡' in title or 'SMS' in title or 'sms' in title:
            subcategory = '×©×•"×ª ×¡××¡'

    # ×—×™×¤×•×© ×‘×§×˜×’×•×¨×™×•×ª XML
    for key, value in priority_map.items():
        for cat in xml_categories:
            if key.lower() in cat.lower():
                subcategory = value
                break
        if subcategory != '×›×œ×œ×™':
            break

    # ×¢×‘×•×¨ ×¡×“×¨×•×ª, × ×—×¤×© ×’× ×‘×›×•×ª×¨×ª
    if category == '×¡×“×¨×•×ª':
        for key, value in priority_map.items():
            if key.lower() in title.lower():
                subcategory = value
                break

    return category, subcategory

def create_database(html_folder_path, xml_path, overwrite_db=False):
    """
    ×™×•×¦×¨ ×“××˜×” ×‘×™×™×¡ ××©×•×œ×‘
    """
    logging.info("ğŸš€ ××ª×—×™×œ ×‘× ×™×™×ª ×“××˜×” ×‘×™×™×¡ ××©×•×œ×‘×ª...")

    db_path = 'aviner_database.db'
    if overwrite_db and os.path.exists(db_path):
        os.remove(db_path)

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # ×™×¦×™×¨×ª ×˜×‘×œ××•×ª
    tables_sql = """
    CREATE TABLE IF NOT EXISTS categories (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        name TEXT UNIQUE NOT NULL,
        description TEXT,
        article_count INTEGER DEFAULT 0
    );
    CREATE TABLE IF NOT EXISTS subcategories (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        name TEXT NOT NULL,
        category_id INTEGER,
        description TEXT,
        article_count INTEGER DEFAULT 0,
        FOREIGN KEY (category_id) REFERENCES categories (id)
    );
    CREATE TABLE IF NOT EXISTS articles (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        title TEXT NOT NULL,
        filename TEXT UNIQUE NOT NULL,
        url_slug TEXT,
        section TEXT,
        category_id INTEGER,
        subcategory_id INTEGER,
        content TEXT,
        content_length INTEGER,
        video_id TEXT,
        link_count INTEGER DEFAULT 0,
        is_hub BOOLEAN DEFAULT 0,
        created_date TEXT,
        FOREIGN KEY (category_id) REFERENCES categories (id),
        FOREIGN KEY (subcategory_id) REFERENCES subcategories (id)
    );
    CREATE INDEX IF NOT EXISTS idx_section ON articles(section);
    CREATE INDEX IF NOT EXISTS idx_category ON articles(category_id);
    CREATE INDEX IF NOT EXISTS idx_subcategory ON articles(subcategory_id);
    CREATE INDEX IF NOT EXISTS idx_filename ON articles(filename);
    """
    cursor.executescript(tables_sql)
    logging.info("âœ… ×˜×‘×œ××•×ª × ×•×¦×¨×•")

    # ×§×¨×™××ª ×§×˜×’×•×¨×™×•×ª ×-XML
    xml_categories = extract_categories_from_xml(xml_path)

    # ×¢×™×‘×•×“ ×§×‘×¦×™ HTML
    html_folder = Path(html_folder_path)
    html_files = list(html_folder.glob('*.html'))
    logging.info(f"ğŸ“‚ × ××¦××• {len(html_files):,} ×§×‘×¦×™ HTML")

    data = []
    categories_dict = defaultdict(int)
    subcategories_dict = defaultdict(int)

    for i, file_path in enumerate(html_files, 1):
        if i % 500 == 0:
            logging.info(f"ğŸ”„ ×¢×™×‘×•×“ ×§×•×‘×¥ {i}/{len(html_files)}")

        filename = file_path.name

        # ×¤×¢× ×•×— ×©× ×”×§×•×‘×¥ ×× ×”×•× ××§×•×“×“
        from urllib.parse import unquote
        try:
            decoded_filename = unquote(filename)
        except:
            decoded_filename = filename

        title = decoded_filename.replace('.html', '').strip()

        # ×—×™×¤×•×© ×§×˜×’×•×¨×™×•×ª XML ×œ×¤×™ ×”×›×•×ª×¨×ª
        xml_cats = xml_categories.get(title, [])

        # ×§×‘×™×¢×ª ×§×˜×’×•×¨×™×” ×•×ª×ª-×§×˜×’×•×¨×™×”
        category, subcategory = categorize_with_xml(title, xml_cats)
        section = category

        try:
            # ×—×©×•×‘: ×§×•×¨××™× ××”-file_path ×”××§×•×¨×™, ×œ× ××”-filename!
            with open(file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            soup = BeautifulSoup(html_content, 'html.parser')
            content_div = soup.find('div', id='mw-content-text') or \
                          soup.find('div', class_='mw-parser-output') or \
                          soup.find('div', class_='mw-content-ltr') or \
                          soup.find('div', class_='mw-content-rtl') or \
                          soup.find('body')
            content = content_div.get_text(separator='\n', strip=True) if content_div else ""

            # Video ID
            video_id = None
            youtube_match = re.search(r'youtube\.com/watch\?v=([^&"\s]+)', html_content)
            if youtube_match:
                video_id = youtube_match.group(1)

            # Link count
            link_count = len(soup.find_all('a'))

            # Hub detection
            is_hub = 1 if link_count > 50 and len(content) < 2000 else 0

            url_slug = re.sub(r'[^\w\s-]', '', title).strip().replace(' ', '_')

            data.append({
                'title': title,
                'filename': filename,
                'url_slug': url_slug,
                'section': section,
                'category': category,
                'subcategory': subcategory,
                'content': content,
                'content_length': len(content),
                'video_id': video_id,
                'link_count': link_count,
                'is_hub': is_hub,
                'created_date': datetime.datetime.now().isoformat()
            })

        except Exception as e:
            logging.warning(f"âš ï¸ ×©×’×™××” ×‘×¢×™×‘×•×“ {filename}: {e}")
            continue

    df = pd.DataFrame(data)

    # ×”×›× ×¡×ª ×§×˜×’×•×¨×™×•×ª
    unique_categories = df['category'].dropna().unique()
    for cat_name in unique_categories:
        cursor.execute("INSERT OR IGNORE INTO categories (name) VALUES (?)", (cat_name,))
        cursor.execute("SELECT id FROM categories WHERE name = ?", (cat_name,))
        categories_dict[cat_name] = cursor.fetchone()[0]

    # ×”×›× ×¡×ª ×ª×ª×™-×§×˜×’×•×¨×™×•×ª
    unique_subcats = df['subcategory'].dropna().unique()
    for subcat_name in unique_subcats:
        cat_name = df[df['subcategory'] == subcat_name]['category'].iloc[0] if not df[df['subcategory'] == subcat_name].empty else None
        cat_id = categories_dict.get(cat_name)
        if cat_id:
            cursor.execute("INSERT OR IGNORE INTO subcategories (name, category_id) VALUES (?, ?)", (subcat_name, cat_id))
            cursor.execute("SELECT id FROM subcategories WHERE name = ? AND category_id = ?", (subcat_name, cat_id))
            subcategories_dict[subcat_name] = cursor.fetchone()[0]

    # ×”×›× ×¡×ª ××××¨×™×
    for _, row in df.iterrows():
        cat_id = categories_dict.get(row['category'])
        subcat_id = subcategories_dict.get(row['subcategory'])
        cursor.execute("""
            INSERT INTO articles (title, filename, url_slug, section, category_id, subcategory_id, content, content_length, video_id, link_count, is_hub, created_date)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (row['title'], row['filename'], row['url_slug'], row['section'], cat_id, subcat_id, row['content'], row['content_length'], row['video_id'], row['link_count'], row['is_hub'], row['created_date']))

    # ×¢×“×›×•×Ÿ ××•× ×™×
    cursor.execute("UPDATE categories SET article_count = (SELECT COUNT(*) FROM articles WHERE category_id = categories.id)")
    cursor.execute("UPDATE subcategories SET article_count = (SELECT COUNT(*) FROM articles WHERE subcategory_id = subcategories.id)")

    conn.commit()

    # ×¡×˜×˜×™×¡×˜×™×§×•×ª
    stats = pd.read_sql_query("""
        SELECT c.name as category, COUNT(DISTINCT s.id) as subcategories_count, COUNT(a.id) as articles_count
        FROM categories c
        LEFT JOIN subcategories s ON c.id = s.category_id
        LEFT JOIN articles a ON c.id = a.category_id
        GROUP BY c.id
        ORDER BY articles_count DESC
    """, conn)
    logging.info("\nğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª ×¡×•×¤×™×•×ª:")
    logging.info(stats.to_string(index=False))

    # ×‘×“×™×§×ª ×ª×•×›×Ÿ
    cursor.execute("SELECT COUNT(*) FROM articles WHERE content_length > 0")
    with_content = cursor.fetchone()[0]
    cursor.execute("SELECT COUNT(*) FROM articles")
    total = cursor.fetchone()[0]

    logging.info(f"\nâœ… ×ª×•×›×Ÿ: {with_content}/{total} ({with_content/total*100:.1f}%)")
    logging.info(f"ğŸ“ ××™×§×•×: {db_path}")
    logging.info(f"ğŸ“ˆ ××××¨×™×: {len(df):,}")

    conn.close()
    return db_path

if __name__ == "__main__":
    xml_path = "backup.xml"
    folder_path = "./pages"
    db_file = create_database(folder_path, xml_path, overwrite_db=True)
