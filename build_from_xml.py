#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
×‘×•× ×” ×“××˜×”-×‘×™×™×¡ ××§×•×‘×¥ backup.xml ×©×œ MediaWiki
"""

import re
import sqlite3
import xml.etree.ElementTree as ET
from pathlib import Path
import logging
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def extract_categories(wikitext):
    """
    ×—×•×œ×¥ ×§×˜×’×•×¨×™×•×ª ××˜×§×¡×˜ ×•×™×§×™
    """
    categories = []
    # Pattern: [[×§×˜×’×•×¨×™×”:×©×_×§×˜×’×•×¨×™×”]] ××• [[Category:×©×]]
    cat_pattern = r'\[\[(?:×§×˜×’×•×¨×™×”|Category):([^\]|]+)(?:\|[^\]]*)?\]\]'
    matches = re.finditer(cat_pattern, wikitext, re.IGNORECASE)
    for match in matches:
        cat_name = match.group(1).strip()
        categories.append(cat_name)
    return categories

def extract_video_id(wikitext):
    """
    ×—×•×œ×¥ YouTube video ID ××˜×§×¡×˜ ×•×™×§×™
    """
    # Pattern: <youtube>VIDEO_ID</youtube>
    video_pattern = r'<youtube>([^<]+)</youtube>'
    match = re.search(video_pattern, wikitext)
    if match:
        return match.group(1).strip()
    return None

def categorize_article(title, categories):
    """
    ×§×•×‘×¢ ××ª ×”×§×˜×’×•×¨×™×” ×”×¨××©×™×ª ×•×ª×ª-×”×§×˜×’×•×¨×™×” ×¢×œ ×¤×™ ×”×›×•×ª×¨×ª ×•×”×§×˜×’×•×¨×™×•×ª
    """
    title_lower = title.lower()

    # ×§×‘×™×¢×ª ×§×˜×’×•×¨×™×” ×¨××©×™×ª ×œ×¤×™ ×”×¡×•×’×¨×™×™× ×‘×›×•×ª×¨×ª
    main_category = '×›×œ×œ×™'
    subcategory = '×›×œ×œ×™'

    if '(×•×™×“××•)' in title or '(×©×™×¢×•×¨)' in title:
        main_category = '×•×™×“××•'
    elif '(××××¨)' in title:
        main_category = '××××¨×™×'
    elif '(×©×•"×ª)' in title or '(×©×•_×ª)' in title:
        main_category = '×©×•"×ª ×”×œ×›×”'
    elif '(×¡×“×¨×”)' in title:
        main_category = '×¡×“×¨×•×ª'

    # ×§×‘×™×¢×ª ×ª×ª-×§×˜×’×•×¨×™×” ×œ×¤×™ ×”×§×˜×’×•×¨×™×•×ª ××”×•×•×™×§×™
    # ×¡×“×¨ ×¢×“×™×¤×•×™×•×ª: ×§×˜×’×•×¨×™×•×ª ×¡×¤×¦×™×¤×™×•×ª ×§×•×“×

    priority_categories = {
        # ×¡×“×¨×•×ª (×‘×“×™×§×” ×¨××©×•× ×” - ×”×›×™ ×¡×¤×¦×™×¤×™)
        '××•×¨×•×ª ×”×§×•×“×©': '××•×¨×•×ª ×”×§×•×“×©',
        '××•×¨×•×ª ×”×ª×©×•×‘×”': '××•×¨×•×ª ×”×ª×©×•×‘×”',
        '×¡×¤×¨ ××•×¨×•×ª': '×¡×¤×¨ ××•×¨×•×ª',
        '×¢×™×Ÿ ××™×”': '×¢×™×Ÿ ××™×”',
        '×›×•×–×¨×™': '×›×•×–×¨×™',
        '×©××•× ×” ×¤×¨×§×™×': '×©××•× ×” ×¤×¨×§×™× ×œ×¨××‘×',
        '×ª×¤××¨×ª ×™×©×¨××œ': '×ª×¤××¨×ª ×™×©×¨××œ - ××”×¨"×œ',

        # × ×•×©××™× ××™×•×—×“×™× (×‘×¢×“×™×¤×•×ª ×’×‘×•×”×”)
        '×©×•"×ª ×¡××¡': '×©×•"×ª ×¡××¡',
        '××§×˜×•××œ×™×”': '××§×˜×•××œ×™×”',

        # × ×•×©××™× ×œ×©×•"×ª (×¢×“×™×¤×•×ª ×¢×œ ×¤× ×™ "×©×•"×ª" ×›×œ×œ×™)
        '×–×•×’×™×•×ª ×•××©×¤×—×”': '×–×•×’×™×•×ª ×•××©×¤×—×”',
        '×–×•×’×™×•×ª': '×–×•×’×™×•×ª ×•××©×¤×—×”',
        '××©×¤×—×”': '×–×•×’×™×•×ª ×•××©×¤×—×”',
        '× ×™×©×•××™×Ÿ': '×–×•×’×™×•×ª ×•××©×¤×—×”',
        '×—×ª×•× ×”': '×–×•×’×™×•×ª ×•××©×¤×—×”',
        '×©×™×“×•×›×™×': '×–×•×’×™×•×ª ×•××©×¤×—×”',

        '××“×™× ×ª ×™×©×¨××œ': '××“×™× ×ª ×™×©×¨××œ',
        '××¨×¥ ×™×©×¨××œ': '××“×™× ×ª ×™×©×¨××œ',
        '×¦×”"×œ': '××“×™× ×ª ×™×©×¨××œ',
        '×¦×‘×': '××“×™× ×ª ×™×©×¨××œ',

        '××•×¡×¨ ×•××™×“×•×ª': '××•×¡×¨ ×•××™×“×•×ª',
        '××•×¡×¨': '××•×¡×¨ ×•××™×“×•×ª',
        '××™×“×•×ª': '××•×¡×¨ ×•××™×“×•×ª',

        '××•×¢×“×™×': '××•×¢×“×™×',
        '×—×’×™×': '××•×¢×“×™×',
        '×©×‘×ª': '××•×¢×“×™×',
        '×¤×¡×—': '××•×¢×“×™×',
        '×¡×•×›×•×ª': '××•×¢×“×™×',
        '×—× ×•×›×”': '××•×¢×“×™×',
        '×¤×•×¨×™×': '××•×¢×“×™×',
        '×¨××© ×”×©× ×”': '××•×¢×“×™×',
        '×™×•× ×›×™×¤×•×¨': '××•×¢×“×™×',

        '×ª×¤×™×œ×”': '×ª×¤×™×œ×”',
        '×ª×¤×™×œ×•×ª': '×ª×¤×™×œ×”',
        '×‘×¨×›×•×ª': '×ª×¤×™×œ×”',

        '×ª×•×¨×”': '×ª×•×¨×”',
        '×œ×™××•×“ ×ª×•×¨×”': '×ª×•×¨×”',
        '×ª×œ××•×“ ×ª×•×¨×”': '×ª×•×¨×”',

        '×—×™× ×•×š': '×—×™× ×•×š',

        '×××•× ×”': '×××•× ×”',
        '×××•× ×” ×•×‘×˜×—×•×Ÿ': '×××•× ×”',

        '××™×•×—×“×™×': '××™×•×—×“×™×',

        # ×”×œ×›×” - ×¨×§ ×× ×œ× ××¦×× ×• ×™×•×ª×¨ ×¡×¤×¦×™×¤×™
        '×”×œ×›×”': '×”×œ×›×”',

        # ××•×¨×•×ª - ×¨×§ ×‘×¡×•×£ (×”×›×™ ×›×œ×œ×™)
        '××•×¨×•×ª': '××•×¨×•×ª',
    }

    # ×—×™×¤×•×© ×ª×ª-×§×˜×’×•×¨×™×” ×œ×¤×™ ×¡×“×¨ ×¢×“×™×¤×•×™×•×ª
    # ×¢×•×‘×¨×™× ×¢×œ ×¡×“×¨ ×”×¢×“×™×¤×•×™×•×ª (××”×¡×¤×¦×™×¤×™ ×œ×›×œ×œ×™)
    for key, value in priority_categories.items():
        # ×‘×•×“×§×™× ×× ×”××¤×ª×— ××•×¤×™×¢ ×‘××—×ª ××”×§×˜×’×•×¨×™×•×ª
        for cat in categories:
            cat_lower = cat.lower()
            if key.lower() in cat_lower:
                subcategory = value
                break
        if subcategory != '×›×œ×œ×™':
            break

    # ×× ×–×• ×¡×“×¨×”, × ×—×¤×© ××ª ×©× ×”×¡×“×¨×” ×‘×›×•×ª×¨×ª
    if main_category == '×¡×“×¨×•×ª':
        if '××•×¨×•×ª ×”×§×•×“×©' in title:
            subcategory = '××•×¨×•×ª ×”×§×•×“×©'
        elif '××•×¨×•×ª ×”×ª×©×•×‘×”' in title:
            subcategory = '××•×¨×•×ª ×”×ª×©×•×‘×”'
        elif '××•×¨×•×ª' in title:
            subcategory = '××•×¨×•×ª'
        elif '×¢×™×Ÿ ××™×”' in title:
            subcategory = '×¢×™×Ÿ ××™×”'
        elif '×›×•×–×¨×™' in title:
            subcategory = '×›×•×–×¨×™'

    return main_category, subcategory

def build_database_from_xml(xml_path, pages_folder, db_path='aviner_database.db'):
    """
    ×‘×•× ×” ×“××˜×”-×‘×™×™×¡ ××§×•×‘×¥ XML
    """
    logging.info("ğŸš€ ××ª×—×™×œ ×‘× ×™×™×ª ×“××˜×” ×‘×™×™×¡ ×-XML...")

    # ××—×™×§×ª DB ×§×™×™×
    if Path(db_path).exists():
        Path(db_path).unlink()

    # ×™×¦×™×¨×ª ×—×™×‘×•×¨ ×œ-DB
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # ×™×¦×™×¨×ª ×˜×‘×œ××•×ª
    tables_sql = """
    CREATE TABLE IF NOT EXISTS categories (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        name TEXT UNIQUE NOT NULL,
        description TEXT,
        article_count INTEGER DEFAULT 0
    );
    CREATE TABLE IF NOT EXISTS subcategories (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        name TEXT NOT NULL,
        category_id INTEGER,
        description TEXT,
        article_count INTEGER DEFAULT 0,
        FOREIGN KEY (category_id) REFERENCES categories (id)
    );
    CREATE TABLE IF NOT EXISTS articles (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        title TEXT NOT NULL,
        filename TEXT UNIQUE NOT NULL,
        url_slug TEXT,
        section TEXT,
        category_id INTEGER,
        subcategory_id INTEGER,
        content TEXT,
        content_length INTEGER,
        video_id TEXT,
        wiki_categories TEXT,
        created_date TEXT,
        FOREIGN KEY (category_id) REFERENCES categories (id),
        FOREIGN KEY (subcategory_id) REFERENCES subcategories (id)
    );
    CREATE INDEX IF NOT EXISTS idx_section ON articles(section);
    CREATE INDEX IF NOT EXISTS idx_category ON articles(category_id);
    CREATE INDEX IF NOT EXISTS idx_subcategory ON articles(subcategory_id);
    CREATE INDEX IF NOT EXISTS idx_filename ON articles(filename);
    """
    cursor.executescript(tables_sql)
    logging.info("âœ… ×˜×‘×œ××•×ª × ×•×¦×¨×•")

    # Parse XML
    logging.info("ğŸ“– ×§×•×¨× ×§×•×‘×¥ XML...")
    tree = ET.parse(xml_path)
    root = tree.getroot()

    # MediaWiki namespace
    ns = {'mw': 'http://www.mediawiki.org/xml/export-0.11/'}

    # ×™×¦×™×¨×ª ××™×¤×•×™ ×©×œ ×§×‘×¦×™ HTML - ××§×•×“×“×™× ×•×œ× ××§×•×“×“×™×
    logging.info("ğŸ—‚ï¸ ×™×•×¦×¨ ××™×¤×•×™ ×©×œ ×§×‘×¦×™ HTML...")
    from urllib.parse import unquote
    html_files_map = {}
    pages_dir = Path(pages_folder)
    for file_path in pages_dir.glob('*.html'):
        # × ×¡×™×•×Ÿ ×œ×¤×¢× ×— ××ª ×©× ×”×§×•×‘×¥
        try:
            decoded_name = unquote(file_path.name)
            html_files_map[decoded_name] = file_path
        except:
            pass
        # ×’× ×©× ×œ× ××§×•×“×“
        html_files_map[file_path.name] = file_path
    logging.info(f"ğŸ“ × ××¦××• {len(html_files_map)} ×§×‘×¦×™ HTML")

    # ××•×¡×¤×™× ××™×“×¢ ×¢×œ ×§×˜×’×•×¨×™×•×ª ×•×ª×ª×™-×§×˜×’×•×¨×™×•×ª
    categories_dict = {}
    subcategories_dict = {}

    pages = root.findall('.//mw:page', ns)
    logging.info(f"ğŸ“„ × ××¦××• {len(pages)} ×¢××•×“×™× ×‘-XML")

    articles_data = []
    processed = 0

    for page in pages:
        title_elem = page.find('mw:title', ns)
        if title_elem is None:
            continue

        title = title_elem.text

        # ××“×œ×’×™× ×¢×œ ×¢××•×“×™ ××¢×¨×›×ª
        if title.startswith('×§×˜×’×•×¨×™×”:') or title.startswith('Category:') or \
           title.startswith('×ª×‘× ×™×ª:') or title.startswith('Template:') or \
           title.startswith('××“×™×” ×•×™×§×™:') or title.startswith('MediaWiki:'):
            continue

        # ××•×¦××™× ××ª ×”×’×¨×¡×” ×”××—×¨×•× ×”
        revisions = page.findall('mw:revision', ns)
        if not revisions:
            continue

        latest_revision = revisions[-1]  # ×”×’×¨×¡×” ×”××—×¨×•× ×”

        text_elem = latest_revision.find('.//mw:text', ns)
        if text_elem is None or text_elem.text is None:
            continue

        wikitext = text_elem.text

        # ×—×™×œ×•×¥ ×§×˜×’×•×¨×™×•×ª
        wiki_categories = extract_categories(wikitext)

        # ×—×™×œ×•×¥ video ID
        video_id = extract_video_id(wikitext)

        # ×§×‘×™×¢×ª ×§×˜×’×•×¨×™×” ×¨××©×™×ª ×•×ª×ª-×§×˜×’×•×¨×™×”
        main_category, subcategory = categorize_article(title, wiki_categories)

        # ×™×¦×™×¨×ª filename ××”×›×•×ª×¨×ª
        filename = title + '.html'
        url_slug = re.sub(r'[^\w\s-]', '', title).strip().replace(' ', '_')

        # ×§×¨×™××ª ×ª×•×›×Ÿ ××§×•×‘×¥ HTML ×× ×§×™×™×
        content = ""
        content_length = 0

        # ×—×™×¤×•×© ×”×§×•×‘×¥ ×‘××™×¤×•×™ - × ×¡×™×•×Ÿ 1: ×©× ××“×•×™×§
        html_path = html_files_map.get(filename)

        # × ×¡×™×•×Ÿ 2: ×—×™×¤×•×© ×’××™×© - ×”×›×•×ª×¨×ª ×¢× ×¡×•×’×¨×™×™×
        if not html_path or not html_path.exists():
            # × × ×¡×” ×œ××¦×•× ×§×•×‘×¥ ×©××ª×—×™×œ ×‘×›×•×ª×¨×ª ×”×–××ª
            title_without_ext = title
            for mapped_filename, mapped_path in html_files_map.items():
                if mapped_filename.startswith(title_without_ext):
                    html_path = mapped_path
                    break

        if html_path and html_path.exists():
            try:
                with open(html_path, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                soup = BeautifulSoup(html_content, 'html.parser')
                content_div = soup.find('div', id='mw-content-text')
                if content_div:
                    content = content_div.get_text(separator='\n', strip=True)
                    content_length = len(content)
            except Exception as e:
                pass  # ×”×ª×¢×œ××•×ª ××©×’×™××•×ª ×§×¨×™××”

        articles_data.append({
            'title': title,
            'filename': filename,
            'url_slug': url_slug,
            'section': main_category,
            'category': main_category,
            'subcategory': subcategory,
            'content': content,
            'content_length': content_length,
            'video_id': video_id,
            'wiki_categories': ', '.join(wiki_categories),
            'created_date': latest_revision.find('mw:timestamp', ns).text if latest_revision.find('mw:timestamp', ns) is not None else None
        })

        processed += 1
        if processed % 500 == 0:
            logging.info(f"ğŸ”„ ×¢×™×‘×•×“ ×¢××•×“ {processed}/{len(pages)}")

    logging.info(f"âœ… ×¢×•×‘×“×• {processed} ×¢××•×“×™×")

    # ×”×›× ×¡×ª ×§×˜×’×•×¨×™×•×ª
    unique_categories = set(article['category'] for article in articles_data)
    for cat_name in unique_categories:
        cursor.execute("INSERT OR IGNORE INTO categories (name) VALUES (?)", (cat_name,))
        cursor.execute("SELECT id FROM categories WHERE name = ?", (cat_name,))
        categories_dict[cat_name] = cursor.fetchone()[0]

    # ×”×›× ×¡×ª ×ª×ª×™-×§×˜×’×•×¨×™×•×ª
    unique_subcats = set((article['subcategory'], article['category']) for article in articles_data)
    for subcat_name, cat_name in unique_subcats:
        cat_id = categories_dict.get(cat_name)
        if cat_id:
            cursor.execute("INSERT OR IGNORE INTO subcategories (name, category_id) VALUES (?, ?)", (subcat_name, cat_id))
            cursor.execute("SELECT id FROM subcategories WHERE name = ? AND category_id = ?", (subcat_name, cat_id))
            result = cursor.fetchone()
            if result:
                subcategories_dict[(subcat_name, cat_name)] = result[0]

    # ×”×›× ×¡×ª ××××¨×™×
    for article in articles_data:
        cat_id = categories_dict.get(article['category'])
        subcat_id = subcategories_dict.get((article['subcategory'], article['category']))

        try:
            cursor.execute("""
                INSERT INTO articles (title, filename, url_slug, section, category_id, subcategory_id,
                                    content, content_length, video_id, wiki_categories, created_date)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (article['title'], article['filename'], article['url_slug'], article['section'],
                  cat_id, subcat_id, article['content'], article['content_length'],
                  article['video_id'], article['wiki_categories'], article['created_date']))
        except sqlite3.IntegrityError:
            # ×›×•×ª×¨×ª ×›×¤×•×œ×” - ××“×œ×’×™×
            pass

    # ×¢×“×›×•×Ÿ ××•× ×™×
    cursor.execute("UPDATE categories SET article_count = (SELECT COUNT(*) FROM articles WHERE category_id = categories.id)")
    cursor.execute("UPDATE subcategories SET article_count = (SELECT COUNT(*) FROM articles WHERE subcategory_id = subcategories.id)")

    conn.commit()

    # ×¡×˜×˜×™×¡×˜×™×§×•×ª
    logging.info("\nğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª ×¡×•×¤×™×•×ª:")
    cursor.execute("""
        SELECT c.name as category, COUNT(DISTINCT s.id) as subcategories_count, COUNT(a.id) as articles_count
        FROM categories c
        LEFT JOIN subcategories s ON c.id = s.category_id
        LEFT JOIN articles a ON c.id = a.category_id
        GROUP BY c.id
        ORDER BY articles_count DESC
    """)

    for row in cursor.fetchall():
        logging.info(f"  {row[0]}: {row[2]} ××××¨×™×, {row[1]} ×ª×ª×™-×§×˜×’×•×¨×™×•×ª")

    total_articles = cursor.execute("SELECT COUNT(*) FROM articles").fetchone()[0]
    logging.info(f"\nğŸ‰ ×“××˜×” ×‘×™×™×¡ ×”×•×©×œ×!")
    logging.info(f"ğŸ“ ××™×§×•×: {db_path}")
    logging.info(f"ğŸ“ˆ ×¡×”\"×› ××××¨×™×: {total_articles:,}")

    conn.close()
    return db_path

if __name__ == "__main__":
    xml_path = "backup.xml"
    pages_folder = "./pages"
    db_file = build_database_from_xml(xml_path, pages_folder)
